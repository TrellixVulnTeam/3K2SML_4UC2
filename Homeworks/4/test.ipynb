{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    NN for binary classification\n",
    "    Attributes:\n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers_d, normalize = True, learning_rate = 0.01, num_iter = 30000, epsilon = (10)^(-10), k = 500):\n",
    "        self.layers_d = layers_d # тут лише приховані шари а 0-го та останнього (з одним нейроном) немає\n",
    "        self.L = len(self.layers_d) + 1 # кількість шарів нейронів в мережі без урахування вихідного\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalize = normalize\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "    \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        \"\"\"\n",
    "        Зверніть увагу, що нормалізація вхідних даних є дуже важливою для швидкодії нейронних мереж.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        X.shape =  (n, m)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.mean(X, axis=1).reshape((n, 1))\n",
    "            '''\n",
    "            m.shape =  (n, 1)\n",
    "            '''\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.std(X, axis=1).reshape((n, 1))\n",
    "            '''\n",
    "            s.shape =  (n, 1)\n",
    "            '''\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        В наступних практичних потрібно буде додати підтримку й інших активаційних функцій - це один з гіперпараметрів. \n",
    "        Їх можна вибирати для всіх шарів одночасно або мати різні активаційні функції на кожному з них.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        \n",
    "        self.parameters = {} # стоврюємо словник зі значеннями W_i та b_i,ключами в якому будуть назви W_1, w_2, ... та b_1, b_2 і т.д\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.parameters['W_' + str(i)] = np.random.randn(self.layers_d[i], self.layers_d[i - 1])* np.sqrt(2/self.layers_d[i - 1])\n",
    "            '''\n",
    "            W_i.shape  = (n_l, n_l-1) # (кількість нейронів на поточному шарі, кількість на попередньому)\n",
    "            '''\n",
    "            self.parameters['b_' + str(i)] = np.zeros((self.layers_d[i],1))\n",
    "            '''\n",
    "            b_i.shape  = (n_l,1) # (кількість нейронів на поточному шарі, 1)\n",
    "            '''\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        \n",
    "        cache = {} # стоврюємо словник зі значеннями Z_i та A_i,ключами в якому будуть назви A_0, A_1, A_2, ... та Z_1, Z_2 і т.д\n",
    "        cache['A_0'] = X\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            cache['Z_' + str(i)] = np.dot(self.parameters['W_' + str(i)], cache['A_' + str(i - 1)]) + self.parameters['b_' + str(i)]\n",
    "            '''\n",
    "            Z_i.shape  = (n_l, 1) = (n_l, n_l-1) * (n_l-1, 1) + (n_l,1)\n",
    "            '''\n",
    "            cache['A_' + str(i)] = self.__sigmoid(cache['Z_' + str(i)])\n",
    "            '''\n",
    "            A_i.shape  = (n_l, 1) = (n_l, 1)\n",
    "            '''       \n",
    "        \n",
    "        return cache['A_' + str(self.L)], cache\n",
    "    \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        '''\n",
    "        J.shape  = sum((1, m) x (1, m) - (1, m) x (1, m)) = sum((1, m)) = (1, 1)\n",
    "        '''\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        \n",
    "        gradients['dZ_' + str(self.L)] = cache['A_' + str(self.L)] - Y\n",
    "        '''\n",
    "        dZ_L.shape  = (1, m) - (1, m) = (1, m)\n",
    "        '''\n",
    "        gradients['dW_' + str(self.L)] = (1/m) * np.dot (gradients['dZ_' + str(self.L)], cache['A_' + str(self.L - 1)].T)\n",
    "        '''\n",
    "        dW_L.shape  = (1, m) * ((n_l-1, m).T) = (1, m) * (m, n_l-1) = (1, n_l-1)\n",
    "        '''\n",
    "        gradients['db_' + str(self.L)] = (1/m) * np.sum(gradients['dZ_' + str(self.L)], axis = 1, keepdims = True)\n",
    "        '''\n",
    "        db_L.shape  = sum((1, m)) = (1, 1)\n",
    "        '''\n",
    "        \n",
    "        for i in range(self.L - 1, 0, -1):\n",
    "            dA_i = np.dot (self.parameters['W_' + str(i + 1)].T, gradients['dZ_' + str(i + 1)])\n",
    "            '''\n",
    "            dA_i.shape  = (n_l-1, n_l)*(n_l, m) = (n_l-1, m)\n",
    "            '''\n",
    "            gradients['dZ_' + str(i)] = np.multiply(dA_i, cache['A_' + str(i)] * (1 - cache['A_' + str(i)]))\n",
    "            '''\n",
    "            dZ_i.shape  = (n_l, m)x(n_l, m) = (n_l, m)\n",
    "            '''\n",
    "            gradients['dW_' + str(i)] = (1/m) * np.dot (gradients['dZ_' + str(i)], cache['A_' + str(i - 1)].T)\n",
    "            '''\n",
    "            dW_i.shape  = (n_l, m)*((n_l-1, n).T) = (n_l, m)*(m, n_l-1) = (n_l, n_l-1)\n",
    "            '''\n",
    "            gradients['db_' + str(i)] = (1/m) * np.sum(gradients['dZ_' + str(i)], axis = 1, keepdims = True)\n",
    "            '''\n",
    "            db_i.shape = sum((n_l, m)) = (n_l, 1)\n",
    "            '''       \n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients):\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.parameters['W_' + str(i)] -= self.learning_rate * gradients['dW_' + str(i)]\n",
    "            '''\n",
    "            W_i.shape  = (n_l, n_l-1) # (кількість нейронів на поточному шарі, кількість на попередньому)\n",
    "            '''\n",
    "            self.parameters['b_' + str(i)] -= self.learning_rate * gradients['db_' + str(i)]\n",
    "            '''\n",
    "            b_i.shape  = (n_l,1) # (кількість нейронів на поточному шарі, 1)\n",
    "            '''\n",
    "        \n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        X, Y = X_vert.T, Y_vert.T\n",
    "        \n",
    "        n_x = X.shape[0] # визначаємо кількість нейронів у вихідному шарі\n",
    "        final_classes = Y.shape[0] # визначаємо кількість нейронів у вихідному шарі\n",
    "        \n",
    "        self.layers_d.insert(0, n_x)\n",
    "        self.layers_d.append(final_classes) \n",
    "        '''\n",
    "        додаємо вхідний та вихідний шари до прихованих \n",
    "        і отримуємо клькість всіх шарів нейронної мережі і кількість нейронів на кожному шарі\n",
    "        '''\n",
    "        \n",
    "        if self.normalize:\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = []\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        n_x = X.shape[0]\n",
    "        \n",
    "        self.__initialize_parameters()\n",
    "        \n",
    "        previous_cost = 0;\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients)\n",
    "\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                costs.append(cost)\n",
    "            if (abs(previous_cost - cost) < self.epsilon):\n",
    "                k = k - 1\n",
    "                if (k == 0):\n",
    "                    break;\n",
    "\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(\"Iteration, *1000\")\n",
    "            plt.show()\n",
    "    \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probs = self.__forward_propagation(X)[0]\n",
    "        return probs.T\n",
    "    \n",
    "    def predict(self, X_vert):\n",
    "        positive_probs = self.predict_proba(X_vert)[:, 0]\n",
    "        return (positive_probs >= 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
