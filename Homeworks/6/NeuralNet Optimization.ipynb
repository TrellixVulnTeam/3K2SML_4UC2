{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(cls, x_1, x_2, ax=None, threshold=0.5, contourf=False):\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x_1.min(), x_1.max(), 100), \n",
    "                           np.linspace(x_2.min(), x_2.max(), 100))\n",
    "\n",
    "    X_pred = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    pred = cls.predict_proba(X_pred)[:, 0]\n",
    "    Z = pred.reshape((100, 100))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contour(xx1, xx2, Z, levels=[threshold], colors='black')\n",
    "    ax.set_xlim((x_1.min(), x_1.max()))\n",
    "    ax.set_ylim((x_2.min(), x_2.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(X[:,0], X[:,1], c=(y == 1), cmap=cm_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, normalize = True, learning_rate = 0.01, num_iter = 30000, eps = 10**-2, beta = 0.999):\n",
    "        self.layer_sizes = hidden_layer_sizes \n",
    "        self.layers_count = len(self.layer_sizes) + 1\n",
    "        self.normalize = normalize \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self.COST_APPEND_T = 1\n",
    "        \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.array([np.mean(X, axis=1)]).T\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.array([np.std(X, axis=1)]).T\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __sigmoid_derivative(self, Z):\n",
    "        return np.multiply(Z, 1 - Z)\n",
    "    \n",
    "    def __softmax(self, Z):       \n",
    "        ex = np.exp(Z)        \n",
    "        return ex / np.sum(ex, axis=0, keepdims = True)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        self.parameters = {}\n",
    "        self.rmsprop = {}\n",
    "        n_i = self.layer_sizes\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(n_i[i], n_i[i - 1]) * np.sqrt(2/n_i[i - 1])\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((n_i[i], 1))\n",
    "            self.rmsprop[f\"SdW{i}\"] = np.zeros((n_i[i], n_i[i - 1]))\n",
    "            self.rmsprop[f\"Sdb{i}\"] = np.zeros((n_i[i], 1))\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        cache = {\"A0\" : X}\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            cache[f\"Z{i}\"] = np.dot(self.parameters[f\"W{i}\"], cache[f\"A{i - 1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            cache[f\"A{i}\"] = self.__softmax(cache[f\"Z{i}\"]) if i == self.layers_count else self.__sigmoid(cache[f\"Z{i}\"])\n",
    "\n",
    "        return cache[f\"A{self.layers_count}\"], cache\n",
    "        \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        for i in reversed(range(1, self.layers_count + 1)):\n",
    "            if i == self.layers_count:\n",
    "                gradients[f\"dZ{i}\"] = cache[f\"A{i}\"] - Y\n",
    "            else:\n",
    "                dAi = np.dot(self.parameters[f\"W{i + 1}\"].T, gradients[f\"dZ{i + 1}\"])\n",
    "                gradients[f\"dZ{i}\"] = np.multiply(dAi, self.__sigmoid_derivative(cache[f\"A{i}\"]))\n",
    "                \n",
    "            gradients[f\"dW{i}\"] = (1/m) * np.dot (gradients[f\"dZ{i}\"], cache[f\"A{i - 1}\"].T)  \n",
    "            gradients[f\"db{i}\"] = (1/m) * np.sum(gradients[f\"dZ{i}\"], axis = 1, keepdims = True)\n",
    "                \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients):\n",
    "        # for i in range(1, self.layers_count + 1):\n",
    "        #     dWi = gradients[f\"dW{i}\"]\n",
    "        #     dbi = gradients[f\"db{i}\"]\n",
    "        #     self.parameters[f\"W{i}\"] -= self.learning_rate * dWi\n",
    "        #     self.parameters[f\"b{i}\"] -= self.learning_rate * dbi\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.rmsprop[f\"SdW{i}\"] = self.beta * self.rmsprop[f\"SdW{i}\"] + (1 - self.beta) * gradients[f\"dW{i}\"]**2\n",
    "            self.rmsprop[f\"Sdb{i}\"] = self.beta * self.rmsprop[f\"Sdb{i}\"] + (1 - self.beta) * gradients[f\"db{i}\"]**2\n",
    "        \n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] -= self.learning_rate * gradients[f\"dW{i}\"] / (np.sqrt(self.rmsprop[f\"SdW{i}\"]) + 1e-8)\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * gradients[f\"db{i}\"] / (np.sqrt(self.rmsprop[f\"Sdb{i}\"]) + 1e-8)\n",
    "    \n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        lb = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) \n",
    "        lb.fit(Y_vert)\n",
    "        X, Y = X_vert.T, lb.transform(Y_vert).T\n",
    "        \n",
    "        self.layer_sizes.insert(0, X.shape[0]) #Input layer\n",
    "        self.layer_sizes.append(Y.shape[0]) #Output layer\n",
    "            \n",
    "        if self.normalize: #Normalize\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "                \n",
    "        self.__initialize_parameters()\n",
    "        \n",
    "        costs = [] #Costs log\n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients)\n",
    "\n",
    "            #OUTPUT\n",
    "            if print_cost and i % 1000 == 0: #Print cost every 1000 iterations\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % self.COST_APPEND_T == 0: #Append cost every 1000 iterations\n",
    "                costs.append(cost)\n",
    "            \n",
    "            if(i>=self.COST_APPEND_T*2):\n",
    "                if(abs(costs[-1] - costs[-2]) < self.eps):\n",
    "                    break\n",
    "\n",
    "        #Plot costs\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(f\"Iteration, *{self.COST_APPEND_T}\")\n",
    "            plt.show()\n",
    "        \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probabilities = self.__forward_propagation(X)[0]\n",
    "        return probabilities.T\n",
    "        \n",
    "    def predict(self, X_vert):\n",
    "        probs = self.predict_proba(X_vert)\n",
    "        results_bin = (probs == probs.max(axis=1)[:, None]).astype(int)\n",
    "        return results_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classifier = NeuralNet(hidden_layer_sizes=[4, 3, 2], normalize = True, learning_rate = 0.05, num_iter = 1000, eps=10**-15, beta=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration: 2.180706082482451\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99238624e-01, 7.32914899e-04, 2.84611827e-05],\n",
       "       [9.99195752e-01, 7.74545180e-04, 2.97027204e-05],\n",
       "       [9.99222541e-01, 7.48530630e-04, 2.89286504e-05],\n",
       "       [9.99195990e-01, 7.74314214e-04, 2.96958747e-05],\n",
       "       [9.99238624e-01, 7.32914879e-04, 2.84611820e-05],\n",
       "       [9.99238624e-01, 7.32914928e-04, 2.84611835e-05],\n",
       "       [9.99238590e-01, 7.32947797e-04, 2.84621676e-05],\n",
       "       [9.99238623e-01, 7.32915582e-04, 2.84612031e-05],\n",
       "       [9.99195296e-01, 7.74988054e-04, 2.97158458e-05],\n",
       "       [9.99206158e-01, 7.64439019e-04, 2.94027328e-05],\n",
       "       [9.99238624e-01, 7.32914853e-04, 2.84611813e-05],\n",
       "       [9.99238619e-01, 7.32919662e-04, 2.84613253e-05],\n",
       "       [9.99195454e-01, 7.74834827e-04, 2.97113048e-05],\n",
       "       [9.99195303e-01, 7.74981631e-04, 2.97156554e-05],\n",
       "       [9.99238624e-01, 7.32914817e-04, 2.84611802e-05],\n",
       "       [9.99238624e-01, 7.32914822e-04, 2.84611803e-05],\n",
       "       [9.99238624e-01, 7.32914841e-04, 2.84611809e-05],\n",
       "       [9.99238624e-01, 7.32914960e-04, 2.84611845e-05],\n",
       "       [9.99238624e-01, 7.32914892e-04, 2.84611824e-05],\n",
       "       [9.99238624e-01, 7.32914887e-04, 2.84611823e-05],\n",
       "       [9.99238624e-01, 7.32915146e-04, 2.84611900e-05],\n",
       "       [9.99238624e-01, 7.32914974e-04, 2.84611849e-05],\n",
       "       [9.99238624e-01, 7.32914879e-04, 2.84611820e-05],\n",
       "       [9.99238614e-01, 7.32924449e-04, 2.84614689e-05],\n",
       "       [9.99238618e-01, 7.32920321e-04, 2.84613451e-05],\n",
       "       [9.99196533e-01, 7.73787079e-04, 2.96802488e-05],\n",
       "       [9.99238623e-01, 7.32916174e-04, 2.84612209e-05],\n",
       "       [9.99238624e-01, 7.32914920e-04, 2.84611833e-05],\n",
       "       [9.99238624e-01, 7.32915007e-04, 2.84611859e-05],\n",
       "       [9.99222539e-01, 7.48531977e-04, 2.89286906e-05],\n",
       "       [9.99200161e-01, 7.70263237e-04, 2.95757281e-05],\n",
       "       [9.99238623e-01, 7.32915484e-04, 2.84612002e-05],\n",
       "       [9.99238624e-01, 7.32914829e-04, 2.84611805e-05],\n",
       "       [9.99238624e-01, 7.32914820e-04, 2.84611803e-05],\n",
       "       [9.99206766e-01, 7.63848750e-04, 2.93851832e-05],\n",
       "       [9.99237896e-01, 7.33621862e-04, 2.84823610e-05],\n",
       "       [9.99238624e-01, 7.32914864e-04, 2.84611816e-05],\n",
       "       [9.99238624e-01, 7.32914865e-04, 2.84611816e-05],\n",
       "       [9.99195302e-01, 7.74981871e-04, 2.97156625e-05],\n",
       "       [9.99238624e-01, 7.32915196e-04, 2.84611915e-05],\n",
       "       [9.99238624e-01, 7.32914937e-04, 2.84611838e-05],\n",
       "       [9.99194952e-01, 7.75322048e-04, 2.97257432e-05],\n",
       "       [9.99198108e-01, 7.72257524e-04, 2.96348942e-05],\n",
       "       [9.99238622e-01, 7.32916681e-04, 2.84612362e-05],\n",
       "       [9.99238623e-01, 7.32915322e-04, 2.84611953e-05],\n",
       "       [9.99195471e-01, 7.74817932e-04, 2.97108041e-05],\n",
       "       [9.99238624e-01, 7.32914883e-04, 2.84611821e-05],\n",
       "       [9.99210885e-01, 7.59848940e-04, 2.92661800e-05],\n",
       "       [9.99238624e-01, 7.32914858e-04, 2.84611814e-05],\n",
       "       [9.99238604e-01, 7.32934272e-04, 2.84617627e-05],\n",
       "       [1.11797720e-03, 9.97251518e-01, 1.63050477e-03],\n",
       "       [1.04023184e-03, 9.97339754e-01, 1.62001402e-03],\n",
       "       [1.00357222e-03, 9.97374848e-01, 1.62158001e-03],\n",
       "       [1.09906527e-03, 9.97279356e-01, 1.62157908e-03],\n",
       "       [9.54790408e-04, 9.97404245e-01, 1.64096503e-03],\n",
       "       [1.09726365e-03, 9.97281505e-01, 1.62123102e-03],\n",
       "       [9.46209032e-04, 9.97409853e-01, 1.64393760e-03],\n",
       "       [1.15085899e-03, 9.97217268e-01, 1.63187301e-03],\n",
       "       [1.06681309e-03, 9.97310722e-01, 1.62246480e-03],\n",
       "       [1.10989269e-03, 9.97266937e-01, 1.62317076e-03],\n",
       "       [1.12881619e-03, 9.97244358e-01, 1.62682626e-03],\n",
       "       [1.00361388e-03, 9.97375241e-01, 1.62114489e-03],\n",
       "       [1.14251226e-03, 9.97227583e-01, 1.62990518e-03],\n",
       "       [9.03052818e-04, 9.97390328e-01, 1.70661910e-03],\n",
       "       [1.16097774e-03, 9.97204302e-01, 1.63472008e-03],\n",
       "       [1.12893291e-03, 9.97238411e-01, 1.63265571e-03],\n",
       "       [8.76430257e-04, 9.97348409e-01, 1.77516056e-03],\n",
       "       [1.14537458e-03, 9.97224065e-01, 1.63056072e-03],\n",
       "       [8.77522398e-04, 9.97245960e-01, 1.87651798e-03],\n",
       "       [1.13854759e-03, 9.97232486e-01, 1.62896663e-03],\n",
       "       [4.25048872e-04, 9.89470965e-01, 1.01039861e-02],\n",
       "       [1.14912509e-03, 9.97217342e-01, 1.63353258e-03],\n",
       "       [8.77068341e-04, 9.97266982e-01, 1.85594952e-03],\n",
       "       [1.03457932e-03, 9.97348527e-01, 1.61689341e-03],\n",
       "       [1.10980428e-03, 9.97261197e-01, 1.62899862e-03],\n",
       "       [1.10467521e-03, 9.97267234e-01, 1.62809052e-03],\n",
       "       [9.87972949e-04, 9.97386416e-01, 1.62561072e-03],\n",
       "       [5.98795869e-04, 9.95049714e-01, 4.35148982e-03],\n",
       "       [9.17551756e-04, 9.97402769e-01, 1.67967950e-03],\n",
       "       [1.16319825e-03, 9.97201944e-01, 1.63485765e-03],\n",
       "       [1.13761234e-03, 9.97233632e-01, 1.62875547e-03],\n",
       "       [1.14796131e-03, 9.97220864e-01, 1.63117485e-03],\n",
       "       [1.14504250e-03, 9.97224475e-01, 1.63048298e-03],\n",
       "       [3.18051632e-04, 9.78835343e-01, 2.08466058e-02],\n",
       "       [9.27765429e-04, 9.97409140e-01, 1.66309466e-03],\n",
       "       [9.58831840e-04, 9.97406285e-01, 1.63488339e-03],\n",
       "       [1.02598024e-03, 9.97354080e-01, 1.61993968e-03],\n",
       "       [1.10385081e-03, 9.97273776e-01, 1.62237270e-03],\n",
       "       [1.04782962e-03, 9.97332766e-01, 1.61940473e-03],\n",
       "       [1.11524134e-03, 9.97260633e-01, 1.62412604e-03],\n",
       "       [1.09486075e-03, 9.97284250e-01, 1.62088888e-03],\n",
       "       [9.52115680e-04, 9.97407815e-01, 1.64006959e-03],\n",
       "       [1.13731063e-03, 9.97234006e-01, 1.62868289e-03],\n",
       "       [1.14996475e-03, 9.97218380e-01, 1.63165535e-03],\n",
       "       [1.11427388e-03, 9.97261795e-01, 1.62393070e-03],\n",
       "       [1.05769111e-03, 9.97321480e-01, 1.62082937e-03],\n",
       "       [1.09448287e-03, 9.97283669e-01, 1.62184796e-03],\n",
       "       [1.07787932e-03, 9.97298419e-01, 1.62370175e-03],\n",
       "       [1.16808977e-03, 9.97195847e-01, 1.63606324e-03],\n",
       "       [1.12705833e-03, 9.97246485e-01, 1.62645663e-03],\n",
       "       [2.56452245e-09, 6.47507201e-04, 9.99352490e-01],\n",
       "       [7.30303538e-09, 1.36924989e-03, 9.98630743e-01],\n",
       "       [2.56870600e-09, 6.48259261e-04, 9.99351738e-01],\n",
       "       [2.57898659e-09, 6.50107157e-04, 9.99349890e-01],\n",
       "       [2.56569362e-09, 6.47717753e-04, 9.99352280e-01],\n",
       "       [2.56470327e-09, 6.47539697e-04, 9.99352458e-01],\n",
       "       [1.27613883e-07, 1.06350888e-02, 9.89364784e-01],\n",
       "       [2.56861428e-09, 6.48242775e-04, 9.99351755e-01],\n",
       "       [3.88352540e-09, 8.70945699e-04, 9.99129050e-01],\n",
       "       [2.57091983e-09, 6.48657301e-04, 9.99351340e-01],\n",
       "       [4.09572871e-09, 9.05585229e-04, 9.99094411e-01],\n",
       "       [2.63901595e-09, 6.60853763e-04, 9.99339144e-01],\n",
       "       [2.57729234e-09, 6.49802543e-04, 9.99350195e-01],\n",
       "       [4.50711419e-09, 9.68908287e-04, 9.99031087e-01],\n",
       "       [3.06707747e-09, 7.35690923e-04, 9.99264306e-01],\n",
       "       [2.57908205e-09, 6.50124336e-04, 9.99349873e-01],\n",
       "       [2.62660846e-09, 6.58653037e-04, 9.99341344e-01],\n",
       "       [2.59210062e-09, 6.52463386e-04, 9.99347534e-01],\n",
       "       [2.56375024e-09, 6.47368365e-04, 9.99352629e-01],\n",
       "       [1.05348191e-08, 1.78036748e-03, 9.98219622e-01],\n",
       "       [2.57086691e-09, 6.48647745e-04, 9.99351350e-01],\n",
       "       [1.05109994e-08, 1.77773214e-03, 9.98222257e-01],\n",
       "       [2.56427362e-09, 6.47462450e-04, 9.99352535e-01],\n",
       "       [1.03680670e-08, 1.76172617e-03, 9.98238263e-01],\n",
       "       [2.59535498e-09, 6.53047678e-04, 9.99346950e-01],\n",
       "       [2.65255140e-09, 6.63295467e-04, 9.99336702e-01],\n",
       "       [3.74887369e-09, 8.49817847e-04, 9.99150178e-01],\n",
       "       [7.26234770e-09, 1.36580236e-03, 9.98634190e-01],\n",
       "       [2.56675082e-09, 6.47907694e-04, 9.99352090e-01],\n",
       "       [3.02814625e-09, 7.29257491e-04, 9.99270739e-01],\n",
       "       [2.56790557e-09, 6.48115314e-04, 9.99351882e-01],\n",
       "       [2.30607072e-08, 3.12914014e-03, 9.96870837e-01],\n",
       "       [2.56583509e-09, 6.47743083e-04, 9.99352254e-01],\n",
       "       [9.46473161e-08, 8.60917537e-03, 9.91390730e-01],\n",
       "       [5.98815239e-08, 6.18678189e-03, 9.93813158e-01],\n",
       "       [2.56591280e-09, 6.47757097e-04, 9.99352240e-01],\n",
       "       [2.57153056e-09, 6.48767101e-04, 9.99351230e-01],\n",
       "       [2.68427877e-09, 6.68959861e-04, 9.99331037e-01],\n",
       "       [3.07458030e-08, 3.84592023e-03, 9.96154049e-01],\n",
       "       [2.60909840e-09, 6.55513353e-04, 9.99344484e-01],\n",
       "       [2.56654615e-09, 6.47871011e-04, 9.99352126e-01],\n",
       "       [2.60846210e-09, 6.55398491e-04, 9.99344599e-01],\n",
       "       [7.30303538e-09, 1.36924989e-03, 9.98630743e-01],\n",
       "       [2.56696769e-09, 6.47946805e-04, 9.99352051e-01],\n",
       "       [2.56696296e-09, 6.47945957e-04, 9.99352051e-01],\n",
       "       [2.57520703e-09, 6.49427713e-04, 9.99350570e-01],\n",
       "       [7.84169479e-09, 1.44081713e-03, 9.98559175e-01],\n",
       "       [2.62315891e-09, 6.58034383e-04, 9.99341963e-01],\n",
       "       [2.58977914e-09, 6.52046406e-04, 9.99347951e-01],\n",
       "       [2.75030485e-09, 6.80693020e-04, 9.99319304e-01]])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probabilities = custom_classifier.predict_proba(X)\n",
    "Y_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = custom_classifier.predict(X)\n",
    "y_hat = np.array(lb.fit(y).inverse_transform(y_hat))\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y, y_hat)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "accbce864bb7d7413f19e5e08086b507c673f6b5c8cfdadf3ac75b6d56ce7a21"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
