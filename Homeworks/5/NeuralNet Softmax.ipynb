{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(cls, x_1, x_2, ax=None, threshold=0.5, contourf=False):\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x_1.min(), x_1.max(), 100), \n",
    "                           np.linspace(x_2.min(), x_2.max(), 100))\n",
    "\n",
    "    X_pred = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    pred = cls.predict_proba(X_pred)[:, 0]\n",
    "    Z = pred.reshape((100, 100))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contour(xx1, xx2, Z, levels=[threshold], colors='black')\n",
    "    ax.set_xlim((x_1.min(), x_1.max()))\n",
    "    ax.set_ylim((x_2.min(), x_2.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(X[:,0], X[:,1], c=(y == 1), cmap=cm_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, normalize = True, learning_rate = 0.01, num_iter = 30000, eps = 10**-2):\n",
    "        self.layer_sizes = hidden_layer_sizes \n",
    "        self.layers_count = len(self.layer_sizes) + 1\n",
    "        self.normalize = normalize \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.COST_APPEND_T = 1\n",
    "        \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.array([np.mean(X, axis=1)]).T\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.array([np.std(X, axis=1)]).T\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __softmax(self, Z):       \n",
    "        ex = np.exp(Z)        \n",
    "        return ex / np.sum(ex, axis=0, keepdims = True)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        self.parameters = {}\n",
    "        n_i = self.layer_sizes\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(n_i[i], n_i[i - 1]) * np.sqrt(2/n_i[i - 1])\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((n_i[i], 1))\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        cache = {\"A0\" : X}\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            cache[f\"Z{i}\"] = np.dot(self.parameters[f\"W{i}\"], cache[f\"A{i - 1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            cache[f\"A{i}\"] = self.__softmax(cache[f\"Z{i}\"])\n",
    "\n",
    "        return cache[f\"A{self.layers_count}\"], cache\n",
    "        \n",
    "\n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        for i in reversed(range(1, self.layers_count + 1)):\n",
    "            if i == self.layers_count:\n",
    "                gradients[f\"dZ{i}\"] = cache[f\"A{i}\"] - Y\n",
    "            else:\n",
    "                dAi = np.dot(self.parameters[f\"W{i + 1}\"].T, gradients[f\"dZ{i + 1}\"])\n",
    "                gradients[f\"dZ{i}\"] = np.multiply(dAi, cache[f\"A{i}\"] * (1 - cache[f\"A{i}\"]))\n",
    "                \n",
    "            gradients[f\"dW{i}\"] = (1/m) * np.dot (gradients[f\"dZ{i}\"], cache[f\"A{i - 1}\"].T)  \n",
    "            gradients[f\"db{i}\"] = (1/m) * np.sum(gradients[f\"dZ{i}\"], axis = 1, keepdims = True)\n",
    "                \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients):\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            dWi = gradients[f\"dW{i}\"]\n",
    "            dbi = gradients[f\"db{i}\"]\n",
    "            self.parameters[f\"W{i}\"] -= self.learning_rate * dWi\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * dbi\n",
    "\n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        lb = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) \n",
    "        lb.fit(Y_vert)\n",
    "        X, Y = X_vert.T, lb.transform(Y_vert).T\n",
    "        \n",
    "        if self.normalize: #Normalize\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = [] #Costs log\n",
    "\n",
    "        if(len(self.layer_sizes) - 1 != self.layers_count):\n",
    "            self.layer_sizes.insert(0, X.shape[0]) #Input layer\n",
    "            self.layer_sizes.append(Y.shape[0]) #Output layer\n",
    "        \n",
    "        self.__initialize_parameters()\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients)\n",
    "\n",
    "            #OUTPUT\n",
    "            if print_cost and i % 1000 == 0: #Print cost every 1000 iterations\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % self.COST_APPEND_T == 0: #Append cost every 1000 iterations\n",
    "                costs.append(cost)\n",
    "            \n",
    "            if(i>=self.COST_APPEND_T*2):\n",
    "                if(abs(costs[-1] - costs[-2]) < self.eps):\n",
    "                    break\n",
    "\n",
    "        #Plot costs\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(f\"Iteration, *{self.COST_APPEND_T}\")\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probabilities = self.__forward_propagation(X)[0]\n",
    "        return probabilities.T\n",
    "        \n",
    "    def predict(self, X_vert):\n",
    "        probs = self.predict_proba(X_vert)\n",
    "        results_bin = (probs == probs.max(axis=1)[:, None]).astype(int)\n",
    "        return results_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classifier = NeuralNet(hidden_layer_sizes=[4, 3, 2], normalize = True, learning_rate = 0.05, num_iter = 100000, eps=10**-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration: 1.9198398896411797\n",
      "1000-th iteration: 1.2139223008220064\n",
      "2000-th iteration: 0.7226181385829656\n",
      "3000-th iteration: 0.5593914929258894\n",
      "4000-th iteration: 0.45333802691297803\n",
      "5000-th iteration: 0.3834826086689932\n",
      "6000-th iteration: 0.3372751181308639\n",
      "7000-th iteration: 0.3044442818791935\n",
      "8000-th iteration: 0.2796017433347269\n",
      "9000-th iteration: 0.25998204478381687\n",
      "10000-th iteration: 0.24402408985665586\n",
      "11000-th iteration: 0.2307603364593581\n",
      "12000-th iteration: 0.2195456947336256\n",
      "13000-th iteration: 0.2099265556297509\n",
      "14000-th iteration: 0.2015724163458994\n",
      "15000-th iteration: 0.19423764053427553\n",
      "16000-th iteration: 0.18773828403703943\n",
      "17000-th iteration: 0.18193630874886016\n",
      "18000-th iteration: 0.17672731133230277\n",
      "19000-th iteration: 0.17203009790183146\n",
      "20000-th iteration: 0.16777740972718816\n",
      "21000-th iteration: 0.16390716511071846\n",
      "22000-th iteration: 0.16035338097548055\n",
      "23000-th iteration: 0.1570362302988184\n",
      "24000-th iteration: 0.15385241610548617\n",
      "25000-th iteration: 0.15067150289344758\n",
      "26000-th iteration: 0.1473509518649178\n",
      "27000-th iteration: 0.143784224631173\n",
      "28000-th iteration: 0.13997243355515693\n",
      "29000-th iteration: 0.13606195651293404\n",
      "30000-th iteration: 0.13229050391611968\n",
      "31000-th iteration: 0.12887530884780674\n",
      "32000-th iteration: 0.12593673172402317\n",
      "33000-th iteration: 0.12349462774791216\n",
      "34000-th iteration: 0.12150558430649999\n",
      "35000-th iteration: 0.11990183600713562\n",
      "36000-th iteration: 0.1186158430999701\n",
      "37000-th iteration: 0.11759123577215046\n",
      "38000-th iteration: 0.11678556089937085\n",
      "39000-th iteration: 0.11616921046820362\n",
      "40000-th iteration: 0.11572261456072322\n",
      "41000-th iteration: 0.11543182694308239\n",
      "42000-th iteration: 0.11528155573746328\n",
      "43000-th iteration: 0.11524581397430785\n",
      "44000-th iteration: 0.11528157499083015\n",
      "45000-th iteration: 0.11533593445416979\n",
      "46000-th iteration: 0.115367728447726\n",
      "47000-th iteration: 0.11536373161393353\n",
      "48000-th iteration: 0.11533305398503253\n",
      "49000-th iteration: 0.11528883686410737\n",
      "50000-th iteration: 0.1152336108364354\n",
      "51000-th iteration: 0.11515568195817533\n",
      "52000-th iteration: 0.11503709599595359\n",
      "53000-th iteration: 0.11486614032592742\n",
      "54000-th iteration: 0.11464321901489269\n",
      "55000-th iteration: 0.11437763742677465\n",
      "56000-th iteration: 0.11408160436583888\n",
      "57000-th iteration: 0.11376630509700761\n",
      "58000-th iteration: 0.11344051836037149\n",
      "59000-th iteration: 0.11311062100951055\n",
      "60000-th iteration: 0.11278105691780305\n",
      "61000-th iteration: 0.11245484550558654\n",
      "62000-th iteration: 0.11213399665355431\n",
      "63000-th iteration: 0.11181981637990966\n",
      "64000-th iteration: 0.11151312264935502\n",
      "65000-th iteration: 0.11121439523724974\n",
      "66000-th iteration: 0.11092387942468653\n",
      "67000-th iteration: 0.11064165797926952\n",
      "68000-th iteration: 0.11036770147231897\n",
      "69000-th iteration: 0.11010190377914954\n",
      "70000-th iteration: 0.10984410739676473\n",
      "71000-th iteration: 0.10959412172000962\n",
      "72000-th iteration: 0.10935173641797136\n",
      "73000-th iteration: 0.10911673138544724\n",
      "74000-th iteration: 0.10888888429891642\n",
      "75000-th iteration: 0.10866797650915971\n",
      "76000-th iteration: 0.10845379780509566\n",
      "77000-th iteration: 0.10824615045431037\n",
      "78000-th iteration: 0.10804485284510222\n",
      "79000-th iteration: 0.10784974301077557\n",
      "80000-th iteration: 0.10766068230288303\n",
      "81000-th iteration: 0.1074775594945531\n",
      "82000-th iteration: 0.10730029564074121\n",
      "83000-th iteration: 0.10712885010687888\n",
      "84000-th iteration: 0.1069632283159279\n",
      "85000-th iteration: 0.1068034919815587\n",
      "86000-th iteration: 0.10664977293376256\n",
      "87000-th iteration: 0.10650229217309558\n",
      "88000-th iteration: 0.1063613866291132\n",
      "89000-th iteration: 0.10622754745122737\n",
      "90000-th iteration: 0.10610147588689309\n",
      "91000-th iteration: 0.10598416655375881\n",
      "92000-th iteration: 0.10587703440559233\n",
      "93000-th iteration: 0.10578211325721257\n",
      "94000-th iteration: 0.10570237498297837\n",
      "95000-th iteration: 0.10564225880661475\n",
      "96000-th iteration: 0.10560857885950325\n",
      "97000-th iteration: 0.10561213529859137\n",
      "98000-th iteration: 0.10567066465766929\n",
      "99000-th iteration: 0.10581430864355737\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99197183e-01, 8.02817337e-04, 2.04716568e-12],\n",
       "       [9.97171488e-01, 2.82851188e-03, 2.46606860e-11],\n",
       "       [9.99197129e-01, 8.02871466e-04, 2.04743835e-12],\n",
       "       [9.99192594e-01, 8.07406253e-04, 2.07034541e-12],\n",
       "       [9.99197243e-01, 8.02757331e-04, 2.04686343e-12],\n",
       "       [9.99197167e-01, 8.02832506e-04, 2.04724209e-12],\n",
       "       [9.99197274e-01, 8.02725962e-04, 2.04670543e-12],\n",
       "       [9.99197197e-01, 8.02802680e-04, 2.04709185e-12],\n",
       "       [8.35995889e-01, 1.64004023e-01, 8.88058033e-08],\n",
       "       [9.99191866e-01, 8.08134028e-04, 2.07403342e-12],\n",
       "       [9.99197106e-01, 8.02893898e-04, 2.04755135e-12],\n",
       "       [9.99197256e-01, 8.02743661e-04, 2.04679458e-12],\n",
       "       [9.98895059e-01, 1.10494084e-03, 3.84771384e-12],\n",
       "       [9.99181220e-01, 8.18780442e-04, 2.12835490e-12],\n",
       "       [9.99196868e-01, 8.03132008e-04, 2.04875104e-12],\n",
       "       [9.99197199e-01, 8.02800516e-04, 2.04708095e-12],\n",
       "       [9.99197135e-01, 8.02865181e-04, 2.04740669e-12],\n",
       "       [9.99197160e-01, 8.02840388e-04, 2.04728180e-12],\n",
       "       [9.99196805e-01, 8.03195163e-04, 2.04906931e-12],\n",
       "       [9.99197249e-01, 8.02751282e-04, 2.04683296e-12],\n",
       "       [9.99196854e-01, 8.03146297e-04, 2.04882305e-12],\n",
       "       [9.99197215e-01, 8.02784562e-04, 2.04700059e-12],\n",
       "       [9.99197286e-01, 8.02713674e-04, 2.04664354e-12],\n",
       "       [9.99196808e-01, 8.03191734e-04, 2.04905202e-12],\n",
       "       [9.99197263e-01, 8.02737121e-04, 2.04676164e-12],\n",
       "       [9.87592776e-01, 1.24072239e-02, 4.61336272e-10],\n",
       "       [9.99197160e-01, 8.02840272e-04, 2.04728121e-12],\n",
       "       [9.99197135e-01, 8.02864948e-04, 2.04740551e-12],\n",
       "       [9.99197061e-01, 8.02938915e-04, 2.04777814e-12],\n",
       "       [9.99197029e-01, 8.02970830e-04, 2.04793893e-12],\n",
       "       [9.99186525e-01, 8.13475094e-04, 2.10119857e-12],\n",
       "       [9.99196587e-01, 8.03413220e-04, 2.05016835e-12],\n",
       "       [9.99197280e-01, 8.02719947e-04, 2.04667514e-12],\n",
       "       [9.99197237e-01, 8.02762866e-04, 2.04689131e-12],\n",
       "       [9.99186127e-01, 8.13872543e-04, 2.10322701e-12],\n",
       "       [9.99196889e-01, 8.03111473e-04, 2.04864757e-12],\n",
       "       [9.99196699e-01, 8.03300714e-04, 2.04960126e-12],\n",
       "       [9.99197268e-01, 8.02731767e-04, 2.04673467e-12],\n",
       "       [9.99091171e-01, 9.08829079e-04, 2.61553753e-12],\n",
       "       [9.99197147e-01, 8.02852536e-04, 2.04734299e-12],\n",
       "       [9.99197200e-01, 8.02799802e-04, 2.04707736e-12],\n",
       "       [5.40649460e-01, 4.59349502e-01, 1.03786679e-06],\n",
       "       [9.99197233e-01, 8.02767436e-04, 2.04691433e-12],\n",
       "       [9.99197155e-01, 8.02845040e-04, 2.04730523e-12],\n",
       "       [9.99197251e-01, 8.02749160e-04, 2.04682228e-12],\n",
       "       [9.93920460e-01, 6.07954003e-03, 1.12091802e-10],\n",
       "       [9.99197260e-01, 8.02740342e-04, 2.04677786e-12],\n",
       "       [9.99197155e-01, 8.02844657e-04, 2.04730330e-12],\n",
       "       [9.99197169e-01, 8.02830598e-04, 2.04723248e-12],\n",
       "       [9.99197143e-01, 8.02857041e-04, 2.04736568e-12],\n",
       "       [2.83306103e-03, 9.96367293e-01, 7.99646294e-04],\n",
       "       [1.30079940e-03, 9.96989601e-01, 1.70959978e-03],\n",
       "       [2.10427210e-04, 9.89841422e-01, 9.94815128e-03],\n",
       "       [1.68465209e-03, 9.96986596e-01, 1.32875154e-03],\n",
       "       [2.13856371e-04, 9.89990577e-01, 9.79556690e-03],\n",
       "       [7.80865745e-04, 9.96411100e-01, 2.80803407e-03],\n",
       "       [1.66661498e-04, 9.87407488e-01, 1.24258501e-02],\n",
       "       [7.63947317e-02, 9.23577511e-01, 2.77568137e-05],\n",
       "       [2.56114929e-03, 9.96556241e-01, 8.82609829e-04],\n",
       "       [2.41683185e-03, 9.96649059e-01, 9.34108741e-04],\n",
       "       [2.76438834e-02, 9.72273382e-01, 8.27344882e-05],\n",
       "       [1.54014637e-03, 9.97009671e-01, 1.45018292e-03],\n",
       "       [2.13834508e-02, 9.78508938e-01, 1.07611357e-04],\n",
       "       [2.38277656e-04, 9.90929421e-01, 8.83230129e-03],\n",
       "       [3.52223756e-02, 9.64713290e-01, 6.43346319e-05],\n",
       "       [5.59641843e-03, 9.93993655e-01, 4.09926785e-04],\n",
       "       [1.55564886e-04, 9.86577555e-01, 1.32668806e-02],\n",
       "       [3.23730153e-02, 9.67556730e-01, 7.02544543e-05],\n",
       "       [1.80101250e-05, 9.07910638e-01, 9.20713517e-02],\n",
       "       [2.02500532e-02, 9.79636211e-01, 1.13735614e-04],\n",
       "       [8.18355953e-07, 4.76181148e-01, 5.23818034e-01],\n",
       "       [1.32030893e-02, 9.86621917e-01, 1.74994146e-04],\n",
       "       [3.32920550e-06, 7.07989913e-01, 2.92006758e-01],\n",
       "       [1.31501988e-03, 9.96993388e-01, 1.69159206e-03],\n",
       "       [7.42389854e-03, 9.92265925e-01, 3.10176601e-04],\n",
       "       [3.77960735e-03, 9.95617499e-01, 6.02893718e-04],\n",
       "       [3.23591891e-04, 9.93093682e-01, 6.58272628e-03],\n",
       "       [2.31261692e-06, 6.49116495e-01, 3.50881192e-01],\n",
       "       [2.29065379e-04, 9.90598669e-01, 9.17226520e-03],\n",
       "       [1.03050874e-01, 8.96929557e-01, 1.95692960e-05],\n",
       "       [2.09629477e-02, 9.78927245e-01, 1.09807254e-04],\n",
       "       [4.61041623e-02, 9.53847445e-01, 4.83925762e-05],\n",
       "       [2.03851093e-02, 9.79501920e-01, 1.12970577e-04],\n",
       "       [5.49183015e-08, 1.59480648e-01, 8.40519297e-01],\n",
       "       [1.04229065e-04, 9.80531408e-01, 1.93643630e-02],\n",
       "       [5.30763824e-04, 9.95386632e-01, 4.08260378e-03],\n",
       "       [5.01043558e-04, 9.95182261e-01, 4.31669589e-03],\n",
       "       [7.01423247e-04, 9.96182426e-01, 3.11615080e-03],\n",
       "       [8.08993305e-03, 9.91625169e-01, 2.84898215e-04],\n",
       "       [3.02253507e-03, 9.96226922e-01, 7.50543150e-04],\n",
       "       [1.33247917e-03, 9.96997521e-01, 1.67000027e-03],\n",
       "       [6.27159377e-04, 9.95899536e-01, 3.47330491e-03],\n",
       "       [1.17636911e-02, 9.88039921e-01, 1.96387579e-04],\n",
       "       [7.18078653e-02, 9.28162362e-01, 2.97731443e-05],\n",
       "       [2.36375077e-03, 9.96681641e-01, 9.54608504e-04],\n",
       "       [1.21228196e-02, 9.87686600e-01, 1.90580703e-04],\n",
       "       [4.70692388e-03, 9.94807036e-01, 4.86039723e-04],\n",
       "       [5.80613555e-03, 9.93798528e-01, 3.95336281e-04],\n",
       "       [1.19112865e-01, 8.80870738e-01, 1.63973116e-05],\n",
       "       [5.60640228e-03, 9.93984390e-01, 4.09207780e-04],\n",
       "       [1.09792015e-12, 8.34409859e-04, 9.99165590e-01],\n",
       "       [6.32931022e-11, 6.15567121e-03, 9.93844329e-01],\n",
       "       [2.73115034e-12, 1.30802977e-03, 9.98691970e-01],\n",
       "       [4.00886992e-11, 4.91651774e-03, 9.95083482e-01],\n",
       "       [1.63510623e-12, 1.01558171e-03, 9.98984418e-01],\n",
       "       [1.23531937e-12, 8.84389114e-04, 9.99115611e-01],\n",
       "       [4.06707495e-08, 1.39180415e-01, 8.60819545e-01],\n",
       "       [4.12959181e-12, 1.60390479e-03, 9.98396095e-01],\n",
       "       [5.79229153e-12, 1.89513945e-03, 9.98104861e-01],\n",
       "       [1.26031139e-12, 8.93171433e-04, 9.99106829e-01],\n",
       "       [1.94110319e-09, 3.28900303e-02, 9.67109968e-01],\n",
       "       [5.28618975e-11, 5.63361335e-03, 9.94366387e-01],\n",
       "       [9.35099492e-12, 2.39991616e-03, 9.97600084e-01],\n",
       "       [1.25210507e-11, 2.77134371e-03, 9.97228656e-01],\n",
       "       [1.68278795e-12, 1.03008513e-03, 9.98969915e-01],\n",
       "       [4.60433094e-12, 1.69232946e-03, 9.98307671e-01],\n",
       "       [2.97133150e-10, 1.31582778e-02, 9.86841722e-01],\n",
       "       [1.68613344e-12, 1.03109486e-03, 9.98968905e-01],\n",
       "       [1.03781437e-12, 8.11551618e-04, 9.99188448e-01],\n",
       "       [4.15856162e-08, 1.40599450e-01, 8.59400508e-01],\n",
       "       [2.17426824e-12, 1.16887706e-03, 9.98831123e-01],\n",
       "       [7.76019892e-11, 6.80488430e-03, 9.93195116e-01],\n",
       "       [1.22621266e-12, 8.81166609e-04, 9.99118833e-01],\n",
       "       [2.36754323e-08, 1.08469337e-01, 8.91530639e-01],\n",
       "       [7.19203848e-12, 2.10858189e-03, 9.97891418e-01],\n",
       "       [5.83760742e-11, 5.91552979e-03, 9.94084470e-01],\n",
       "       [1.30746918e-07, 2.33537575e-01, 7.66462294e-01],\n",
       "       [1.13659887e-07, 2.19895346e-01, 7.80104540e-01],\n",
       "       [2.54113087e-12, 1.26232096e-03, 9.98737679e-01],\n",
       "       [6.35121162e-09, 5.82522936e-02, 9.41747700e-01],\n",
       "       [4.03720308e-12, 1.58610680e-03, 9.98413893e-01],\n",
       "       [3.92206016e-09, 4.62148406e-02, 9.53785155e-01],\n",
       "       [1.76675122e-12, 1.05512674e-03, 9.98944873e-01],\n",
       "       [2.68914279e-06, 6.73862105e-01, 3.26135206e-01],\n",
       "       [5.42159056e-09, 5.39984475e-02, 9.46001547e-01],\n",
       "       [1.41788570e-12, 9.46620147e-04, 9.99053380e-01],\n",
       "       [1.69724873e-12, 1.03444240e-03, 9.98965558e-01],\n",
       "       [4.07079609e-10, 1.53530363e-02, 9.84646963e-01],\n",
       "       [2.79343822e-07, 3.19771087e-01, 6.80228633e-01],\n",
       "       [3.14223843e-11, 4.36083041e-03, 9.95639170e-01],\n",
       "       [1.56246936e-12, 9.93069158e-04, 9.99006931e-01],\n",
       "       [2.15108652e-11, 3.61826896e-03, 9.96381731e-01],\n",
       "       [6.32931022e-11, 6.15567121e-03, 9.93844329e-01],\n",
       "       [1.49843473e-12, 9.72778546e-04, 9.99027221e-01],\n",
       "       [1.32874787e-12, 9.16778287e-04, 9.99083222e-01],\n",
       "       [6.20458731e-12, 1.96049614e-03, 9.98039504e-01],\n",
       "       [2.11136008e-10, 1.11278407e-02, 9.88872159e-01],\n",
       "       [1.74020421e-10, 1.01203080e-02, 9.89879692e-01],\n",
       "       [3.95305966e-12, 1.56971657e-03, 9.98430283e-01],\n",
       "       [3.84125139e-09, 4.57535900e-02, 9.54246406e-01]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probabilities = custom_classifier.predict_proba(X)\n",
    "Y_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = custom_classifier.predict(X)\n",
    "y_hat = np.array(lb.fit(y).inverse_transform(y_hat))\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y, y_hat)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "accbce864bb7d7413f19e5e08086b507c673f6b5c8cfdadf3ac75b6d56ce7a21"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
