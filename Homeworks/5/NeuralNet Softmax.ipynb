{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(cls, x_1, x_2, ax=None, threshold=0.5, contourf=False):\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x_1.min(), x_1.max(), 100), \n",
    "                           np.linspace(x_2.min(), x_2.max(), 100))\n",
    "\n",
    "    X_pred = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    pred = cls.predict_proba(X_pred)[:, 0]\n",
    "    Z = pred.reshape((100, 100))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contour(xx1, xx2, Z, levels=[threshold], colors='black')\n",
    "    ax.set_xlim((x_1.min(), x_1.max()))\n",
    "    ax.set_ylim((x_2.min(), x_2.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(X[:,0], X[:,1], c=(y == 1), cmap=cm_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, normalize = True, learning_rate = 0.01, num_iter = 30000, eps = 10**-2):\n",
    "        self.layer_sizes = hidden_layer_sizes \n",
    "        self.layers_count = len(self.layer_sizes) + 1\n",
    "        self.normalize = normalize \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.COST_APPEND_T = 1\n",
    "        \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.array([np.mean(X, axis=1)]).T\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.array([np.std(X, axis=1)]).T\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __softmax(self, Z):       \n",
    "        ex = np.exp(Z)        \n",
    "        return ex / np.sum(ex, axis=0, keepdims = True)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        self.parameters = {}\n",
    "        n_i = self.layer_sizes\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(n_i[i], n_i[i - 1]) * np.sqrt(2/n_i[i - 1])\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((n_i[i], 1))\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        cache = {\"A0\" : X}\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            cache[f\"Z{i}\"] = np.dot(self.parameters[f\"W{i}\"], cache[f\"A{i - 1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            cache[f\"A{i}\"] = self.__softmax(cache[f\"Z{i}\"]) if i == self.layers_count else self.__sigmoid(cache[f\"Z{i}\"])\n",
    "\n",
    "        return cache[f\"A{self.layers_count}\"], cache\n",
    "        \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        for i in reversed(range(1, self.layers_count + 1)):\n",
    "            if i == self.layers_count:\n",
    "                gradients[f\"dZ{i}\"] = cache[f\"A{i}\"] - Y\n",
    "            else:\n",
    "                dAi = np.dot(self.parameters[f\"W{i + 1}\"].T, gradients[f\"dZ{i + 1}\"])\n",
    "                gradients[f\"dZ{i}\"] = np.multiply(dAi, cache[f\"A{i}\"] * (1 - cache[f\"A{i}\"]))\n",
    "                \n",
    "            gradients[f\"dW{i}\"] = (1/m) * np.dot (gradients[f\"dZ{i}\"], cache[f\"A{i - 1}\"].T)  \n",
    "            gradients[f\"db{i}\"] = (1/m) * np.sum(gradients[f\"dZ{i}\"], axis = 1, keepdims = True)\n",
    "                \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients):\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            dWi = gradients[f\"dW{i}\"]\n",
    "            dbi = gradients[f\"db{i}\"]\n",
    "            self.parameters[f\"W{i}\"] -= self.learning_rate * dWi\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * dbi\n",
    "\n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        lb = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) \n",
    "        lb.fit(Y_vert)\n",
    "        X, Y = X_vert.T, lb.transform(Y_vert).T\n",
    "        \n",
    "        if self.normalize: #Normalize\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = [] #Costs log\n",
    "\n",
    "        if(len(self.layer_sizes) - 1 != self.layers_count):\n",
    "            self.layer_sizes.insert(0, X.shape[0]) #Input layer\n",
    "            self.layer_sizes.append(Y.shape[0]) #Output layer\n",
    "        \n",
    "        self.__initialize_parameters()\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients)\n",
    "\n",
    "            #OUTPUT\n",
    "            if print_cost and i % 1000 == 0: #Print cost every 1000 iterations\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % self.COST_APPEND_T == 0: #Append cost every 1000 iterations\n",
    "                costs.append(cost)\n",
    "            \n",
    "            if(i>=self.COST_APPEND_T*2):\n",
    "                if(abs(costs[-1] - costs[-2]) < self.eps):\n",
    "                    break\n",
    "\n",
    "        #Plot costs\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(f\"Iteration, *{self.COST_APPEND_T}\")\n",
    "            plt.show()\n",
    "        \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probabilities = self.__forward_propagation(X)[0]\n",
    "        return probabilities.T\n",
    "        \n",
    "    def predict(self, X_vert):\n",
    "        probs = self.predict_proba(X_vert)\n",
    "        results_bin = (probs == probs.max(axis=1)[:, None]).astype(int)\n",
    "        return results_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classifier = NeuralNet(hidden_layer_sizes=[4, 3, 2], normalize = True, learning_rate = 0.05, num_iter = 100000, eps=10**-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration: 2.1161830650645834\n",
      "1000-th iteration: 1.9079712265190893\n",
      "2000-th iteration: 1.8869782961533164\n",
      "3000-th iteration: 1.5377731727163269\n",
      "4000-th iteration: 1.0068430945272122\n",
      "5000-th iteration: 0.9029900347873431\n",
      "6000-th iteration: 0.6247732074022339\n",
      "7000-th iteration: 0.3505504723093186\n",
      "8000-th iteration: 0.23158528092672628\n",
      "9000-th iteration: 0.1767352638489276\n",
      "10000-th iteration: 0.14657061217267509\n",
      "11000-th iteration: 0.12829641145797027\n",
      "12000-th iteration: 0.11654110288161793\n",
      "13000-th iteration: 0.10859116152993425\n",
      "14000-th iteration: 0.10296463573588045\n",
      "15000-th iteration: 0.09881613130127062\n",
      "16000-th iteration: 0.09564403110415874\n",
      "17000-th iteration: 0.09313907821142055\n",
      "18000-th iteration: 0.0911037509568893\n",
      "19000-th iteration: 0.08940788734110841\n",
      "20000-th iteration: 0.08796335158710827\n",
      "21000-th iteration: 0.08670903352890728\n",
      "22000-th iteration: 0.08560165765253934\n",
      "23000-th iteration: 0.08460997691383368\n",
      "24000-th iteration: 0.08371100659752584\n",
      "25000-th iteration: 0.08288752689348118\n",
      "26000-th iteration: 0.08212639710203083\n",
      "27000-th iteration: 0.08141740204349972\n",
      "28000-th iteration: 0.0807524547612216\n",
      "29000-th iteration: 0.08012504168906392\n",
      "30000-th iteration: 0.07952983473949438\n",
      "31000-th iteration: 0.07896241901977101\n",
      "32000-th iteration: 0.07841910064739471\n",
      "33000-th iteration: 0.07789676964511043\n",
      "34000-th iteration: 0.07739280009044604\n",
      "35000-th iteration: 0.07690497477395485\n",
      "36000-th iteration: 0.07643142534222792\n",
      "37000-th iteration: 0.07597058174471118\n",
      "38000-th iteration: 0.07552112705162066\n",
      "39000-th iteration: 0.0750819555049282\n",
      "40000-th iteration: 0.07465213304659228\n",
      "41000-th iteration: 0.07423086052714198\n",
      "42000-th iteration: 0.07381744032302416\n",
      "43000-th iteration: 0.0734112472160903\n",
      "44000-th iteration: 0.07301170420856158\n",
      "45000-th iteration: 0.07261826360583652\n",
      "46000-th iteration: 0.07223039335364609\n",
      "47000-th iteration: 0.07184756839214157\n",
      "48000-th iteration: 0.07146926675667707\n",
      "49000-th iteration: 0.07109497031532429\n",
      "50000-th iteration: 0.07072417032521346\n",
      "51000-th iteration: 0.07035637829583533\n",
      "52000-th iteration: 0.06999114279806747\n",
      "53000-th iteration: 0.06962807264240427\n",
      "54000-th iteration: 0.06926686605438843\n",
      "55000-th iteration: 0.06890734397242515\n",
      "56000-th iteration: 0.06854948349422216\n",
      "57000-th iteration: 0.06819344531117542\n",
      "58000-th iteration: 0.06783958762838982\n",
      "59000-th iteration: 0.06748845966046108\n",
      "60000-th iteration: 0.06714077096776376\n",
      "61000-th iteration: 0.06679733822475667\n",
      "62000-th iteration: 0.06645901682856038\n",
      "63000-th iteration: 0.0661266288293108\n",
      "64000-th iteration: 0.0658008993652622\n",
      "65000-th iteration: 0.06548241097785425\n",
      "66000-th iteration: 0.06517158019141214\n",
      "67000-th iteration: 0.06486865552553721\n",
      "68000-th iteration: 0.0645737322682304\n",
      "69000-th iteration: 0.0642867775769049\n",
      "70000-th iteration: 0.06400765962230616\n",
      "71000-th iteration: 0.06373617587343207\n",
      "72000-th iteration: 0.06347207747274225\n",
      "73000-th iteration: 0.0632150883790304\n",
      "74000-th iteration: 0.06296491924006793\n",
      "75000-th iteration: 0.06272127671919357\n",
      "76000-th iteration: 0.062483869314645404\n",
      "77000-th iteration: 0.0622524107177446\n",
      "78000-th iteration: 0.06202662159625184\n",
      "79000-th iteration: 0.061806230469430276\n",
      "80000-th iteration: 0.06159097412748941\n",
      "81000-th iteration: 0.061380597871584903\n",
      "82000-th iteration: 0.06117485572001054\n",
      "83000-th iteration: 0.06097351063809982\n",
      "84000-th iteration: 0.06077633479547595\n",
      "85000-th iteration: 0.060583109825804636\n",
      "86000-th iteration: 0.06039362705340984\n",
      "87000-th iteration: 0.06020768765177445\n",
      "88000-th iteration: 0.060025102706307745\n",
      "89000-th iteration: 0.0598456931642076\n",
      "90000-th iteration: 0.059669289665240446\n",
      "91000-th iteration: 0.05949573225708591\n",
      "92000-th iteration: 0.059324870006584575\n",
      "93000-th iteration: 0.05915656052339785\n",
      "94000-th iteration: 0.058990669415291144\n",
      "95000-th iteration: 0.05882706969482178\n",
      "96000-th iteration: 0.05866564115612457\n",
      "97000-th iteration: 0.05850626973828454\n",
      "98000-th iteration: 0.058348846888921925\n",
      "99000-th iteration: 0.05819326893852906\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98986082e-01, 9.61209770e-04, 5.27080610e-05],\n",
       "       [9.98972328e-01, 9.74517988e-04, 5.31540610e-05],\n",
       "       [9.98976289e-01, 9.70685112e-04, 5.30258530e-05],\n",
       "       [9.98955736e-01, 9.90575244e-04, 5.36890481e-05],\n",
       "       [9.98985466e-01, 9.61805511e-04, 5.27280782e-05],\n",
       "       [9.98985544e-01, 9.61730910e-04, 5.27255722e-05],\n",
       "       [9.98967092e-01, 9.79584954e-04, 5.33232485e-05],\n",
       "       [9.98981383e-01, 9.65756098e-04, 5.28606907e-05],\n",
       "       [9.98932987e-01, 1.01259557e-03, 5.44172614e-05],\n",
       "       [9.98974844e-01, 9.72083423e-04, 5.30726484e-05],\n",
       "       [9.98988979e-01, 9.58407191e-04, 5.26138329e-05],\n",
       "       [9.98969655e-01, 9.77104073e-04, 5.32404539e-05],\n",
       "       [9.98971964e-01, 9.74870631e-04, 5.31658475e-05],\n",
       "       [9.98963943e-01, 9.82631922e-04, 5.34248236e-05],\n",
       "       [9.98992476e-01, 9.55024190e-04, 5.24999482e-05],\n",
       "       [9.98987639e-01, 9.59703907e-04, 5.26574495e-05],\n",
       "       [9.98989538e-01, 9.57866296e-04, 5.25956356e-05],\n",
       "       [9.98984973e-01, 9.62282568e-04, 5.27441020e-05],\n",
       "       [9.98988280e-01, 9.59083778e-04, 5.26365908e-05],\n",
       "       [9.98985518e-01, 9.61755392e-04, 5.27263950e-05],\n",
       "       [9.98983708e-01, 9.63506957e-04, 5.27852159e-05],\n",
       "       [9.98983613e-01, 9.63598902e-04, 5.27883048e-05],\n",
       "       [9.98985978e-01, 9.61310405e-04, 5.27114446e-05],\n",
       "       [9.98970296e-01, 9.76484580e-04, 5.32197613e-05],\n",
       "       [9.98940684e-01, 1.00514400e-03, 5.41715330e-05],\n",
       "       [9.98966306e-01, 9.80345583e-04, 5.33486162e-05],\n",
       "       [9.98974990e-01, 9.71941663e-04, 5.30679040e-05],\n",
       "       [9.98985746e-01, 9.61535428e-04, 5.27190031e-05],\n",
       "       [9.98986136e-01, 9.61158040e-04, 5.27063219e-05],\n",
       "       [9.98958629e-01, 9.87775342e-04, 5.35960065e-05],\n",
       "       [9.98963140e-01, 9.83409319e-04, 5.34507165e-05],\n",
       "       [9.98983867e-01, 9.63352824e-04, 5.27800405e-05],\n",
       "       [9.98989875e-01, 9.57540582e-04, 5.25846764e-05],\n",
       "       [9.98991626e-01, 9.55846628e-04, 5.25276501e-05],\n",
       "       [9.98972882e-01, 9.73981956e-04, 5.31361414e-05],\n",
       "       [9.98984514e-01, 9.62726506e-04, 5.27590112e-05],\n",
       "       [9.98989457e-01, 9.57944945e-04, 5.25982802e-05],\n",
       "       [9.98984551e-01, 9.62691268e-04, 5.27578305e-05],\n",
       "       [9.98952179e-01, 9.94017765e-04, 5.38033050e-05],\n",
       "       [9.98983313e-01, 9.63888717e-04, 5.27980322e-05],\n",
       "       [9.98985321e-01, 9.61946552e-04, 5.27328153e-05],\n",
       "       [9.94723645e-01, 5.12946644e-03, 1.46888670e-04],\n",
       "       [9.98957499e-01, 9.88868615e-04, 5.36323506e-05],\n",
       "       [9.98972242e-01, 9.74600993e-04, 5.31568327e-05],\n",
       "       [9.98970443e-01, 9.76342096e-04, 5.32150092e-05],\n",
       "       [9.98967306e-01, 9.79377499e-04, 5.33163259e-05],\n",
       "       [9.98984612e-01, 9.62632250e-04, 5.27558487e-05],\n",
       "       [9.98966212e-01, 9.80435927e-04, 5.33516281e-05],\n",
       "       [9.98988291e-01, 9.59073052e-04, 5.26362304e-05],\n",
       "       [9.98982439e-01, 9.64734936e-04, 5.28264319e-05],\n",
       "       [3.86413751e-03, 9.95695919e-01, 4.39943321e-04],\n",
       "       [8.95927380e-04, 9.98840335e-01, 2.63738078e-04],\n",
       "       [4.89405563e-04, 9.99243940e-01, 2.66654585e-04],\n",
       "       [4.48992721e-04, 9.93824910e-01, 5.72609750e-03],\n",
       "       [4.36936988e-04, 9.99202065e-01, 3.60997787e-04],\n",
       "       [4.51432690e-04, 9.93182641e-01, 6.36592593e-03],\n",
       "       [6.25731322e-04, 9.99111903e-01, 2.62365595e-04],\n",
       "       [4.26853559e-04, 9.99129530e-01, 4.43616760e-04],\n",
       "       [5.29434568e-04, 9.99235351e-01, 2.35213971e-04],\n",
       "       [4.38018998e-04, 9.97273582e-01, 2.28839852e-03],\n",
       "       [4.38200471e-04, 9.96959621e-01, 2.60217896e-03],\n",
       "       [4.62092795e-04, 9.99247008e-01, 2.90898949e-04],\n",
       "       [4.86861568e-04, 9.99274384e-01, 2.38754134e-04],\n",
       "       [4.49205423e-04, 9.94554207e-01, 4.99658800e-03],\n",
       "       [1.19012686e-03, 9.98524719e-01, 2.85154627e-04],\n",
       "       [3.46352823e-03, 9.96114660e-01, 4.21812118e-04],\n",
       "       [4.37106379e-04, 9.98225055e-01, 1.33783903e-03],\n",
       "       [4.43374594e-04, 9.99264982e-01, 2.91643187e-04],\n",
       "       [4.41292539e-04, 9.96383816e-01, 3.17489171e-03],\n",
       "       [4.27840227e-04, 9.99160802e-01, 4.11357304e-04],\n",
       "       [4.80124209e-04, 9.77443146e-01, 2.20767295e-02],\n",
       "       [6.43248220e-04, 9.99120641e-01, 2.36110652e-04],\n",
       "       [4.48418291e-04, 8.75384391e-01, 1.24167191e-01],\n",
       "       [4.36737410e-04, 9.97435389e-01, 2.12787397e-03],\n",
       "       [6.88707739e-04, 9.99071204e-01, 2.40087811e-04],\n",
       "       [9.51918935e-04, 9.98783474e-01, 2.64606860e-04],\n",
       "       [4.77736680e-04, 9.99270621e-01, 2.51642241e-04],\n",
       "       [4.57067998e-04, 8.87707907e-01, 1.11835025e-01],\n",
       "       [4.43497301e-04, 9.96503841e-01, 3.05266193e-03],\n",
       "       [1.09156305e-03, 9.98632720e-01, 2.75717033e-04],\n",
       "       [4.26073047e-04, 9.99106747e-01, 4.67179914e-04],\n",
       "       [4.40225193e-04, 9.99262501e-01, 2.97273920e-04],\n",
       "       [4.76845470e-04, 9.99276807e-01, 2.46347852e-04],\n",
       "       [5.21961750e-05, 8.52263039e-02, 9.14721500e-01],\n",
       "       [4.35290936e-04, 9.98648425e-01, 9.16283837e-04],\n",
       "       [2.34099229e-03, 9.97289012e-01, 3.69995703e-04],\n",
       "       [5.25618716e-04, 9.99226281e-01, 2.48100557e-04],\n",
       "       [4.34366272e-04, 9.99236632e-01, 3.29001930e-04],\n",
       "       [5.26595987e-04, 9.99230205e-01, 2.43198881e-04],\n",
       "       [4.37959536e-04, 9.97088248e-01, 2.47379243e-03],\n",
       "       [4.63328995e-04, 9.81339206e-01, 1.81974655e-02],\n",
       "       [4.31627926e-04, 9.98802998e-01, 7.65374229e-04],\n",
       "       [4.36883066e-04, 9.99245780e-01, 3.17336683e-04],\n",
       "       [4.28046772e-04, 9.99171174e-01, 4.00778836e-04],\n",
       "       [4.37540104e-04, 9.97272110e-01, 2.29035026e-03],\n",
       "       [5.14136849e-04, 9.99242049e-01, 2.43813898e-04],\n",
       "       [4.33574058e-04, 9.99160538e-01, 4.05888166e-04],\n",
       "       [4.94452829e-04, 9.99262823e-01, 2.42723932e-04],\n",
       "       [6.34542947e-04, 9.99130634e-01, 2.34823484e-04],\n",
       "       [4.28838706e-04, 9.99101599e-01, 4.69561904e-04],\n",
       "       [7.03720138e-07, 9.55046345e-04, 9.99044250e-01],\n",
       "       [4.24281697e-06, 6.19219195e-03, 9.93803565e-01],\n",
       "       [7.78783022e-07, 1.06032964e-03, 9.98938892e-01],\n",
       "       [2.23675239e-06, 3.17569563e-03, 9.96822068e-01],\n",
       "       [7.49435159e-07, 1.01941026e-03, 9.98979840e-01],\n",
       "       [7.19571900e-07, 9.77560606e-04, 9.99021720e-01],\n",
       "       [8.24476379e-05, 1.37782235e-01, 8.62135318e-01],\n",
       "       [1.09361064e-06, 1.50789724e-03, 9.98491009e-01],\n",
       "       [3.80939008e-06, 5.53579944e-03, 9.94460391e-01],\n",
       "       [7.95958094e-07, 1.08105305e-03, 9.98918151e-01],\n",
       "       [1.65083768e-06, 2.30422427e-03, 9.97694125e-01],\n",
       "       [2.90797986e-06, 4.17438475e-03, 9.95822707e-01],\n",
       "       [8.56274656e-07, 1.16931127e-03, 9.98829832e-01],\n",
       "       [3.97224111e-06, 5.78319627e-03, 9.94212831e-01],\n",
       "       [7.85177041e-07, 1.06970277e-03, 9.98929512e-01],\n",
       "       [7.68454709e-07, 1.04504664e-03, 9.98954185e-01],\n",
       "       [2.05502436e-06, 2.90527550e-03, 9.97092669e-01],\n",
       "       [9.60742525e-07, 1.30789334e-03, 9.98691146e-01],\n",
       "       [7.09959472e-07, 9.64326119e-04, 9.99034964e-01],\n",
       "       [8.02452898e-05, 1.33967690e-01, 8.65952065e-01],\n",
       "       [7.19494576e-07, 9.77094750e-04, 9.99022186e-01],\n",
       "       [2.85043079e-06, 4.08794693e-03, 9.95909203e-01],\n",
       "       [7.86260262e-07, 1.07116211e-03, 9.98928052e-01],\n",
       "       [4.64422314e-05, 7.52814064e-02, 9.24672151e-01],\n",
       "       [8.06095361e-07, 1.09762048e-03, 9.98901573e-01],\n",
       "       [1.01139562e-06, 1.38900045e-03, 9.98609988e-01],\n",
       "       [4.91001900e-05, 7.97483745e-02, 9.20202525e-01],\n",
       "       [1.82371572e-05, 2.82724286e-02, 9.71709334e-01],\n",
       "       [9.37217116e-07, 1.28498052e-03, 9.98714082e-01],\n",
       "       [1.63531968e-05, 2.52332230e-02, 9.74750424e-01],\n",
       "       [1.20856682e-06, 1.67203475e-03, 9.98326757e-01],\n",
       "       [4.05091658e-06, 5.71615722e-03, 9.94279792e-01],\n",
       "       [8.24865050e-07, 1.12572040e-03, 9.98873455e-01],\n",
       "       [2.80222315e-04, 5.04908172e-01, 4.94811606e-01],\n",
       "       [4.22927971e-05, 6.84331542e-02, 9.31524553e-01],\n",
       "       [7.60915758e-07, 1.03449337e-03, 9.98964746e-01],\n",
       "       [8.40918909e-07, 1.14394088e-03, 9.98855218e-01],\n",
       "       [2.00955417e-06, 2.83664063e-03, 9.97161350e-01],\n",
       "       [3.87511260e-05, 6.21193412e-02, 9.37841908e-01],\n",
       "       [9.04231163e-07, 1.23621896e-03, 9.98762877e-01],\n",
       "       [7.06300319e-07, 9.58913573e-04, 9.99040380e-01],\n",
       "       [9.30762395e-07, 1.27220735e-03, 9.98726862e-01],\n",
       "       [4.24281697e-06, 6.19219195e-03, 9.93803565e-01],\n",
       "       [7.06173784e-07, 9.58723472e-04, 9.99040570e-01],\n",
       "       [7.05599425e-07, 9.57602286e-04, 9.99041692e-01],\n",
       "       [7.91259369e-07, 1.07736092e-03, 9.98921848e-01],\n",
       "       [1.29408862e-05, 1.98180089e-02, 9.80169050e-01],\n",
       "       [1.24739076e-06, 1.72710531e-03, 9.98271647e-01],\n",
       "       [1.11460133e-06, 1.52592105e-03, 9.98472964e-01],\n",
       "       [8.66434589e-06, 1.30140829e-02, 9.86977253e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probabilities = custom_classifier.predict_proba(X)\n",
    "y_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = custom_classifier.predict(X)\n",
    "y_hat = np.array(lb.fit(y).inverse_transform(y_hat))\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y, y_hat)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "accbce864bb7d7413f19e5e08086b507c673f6b5c8cfdadf3ac75b6d56ce7a21"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
